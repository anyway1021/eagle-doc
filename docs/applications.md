# HDFS Data Activity Monitoring
`Placeholder for topic: HDFS Data Activity Monitoring`

---

# JMX Monitoring
`Placeholder for topic: JMX Monitoring`

---

# Job Performance Monitoring

## Monitor Requirements

* Finished/Running Job Details
* Job Metrics(Job Counter/Statistics) Aggregation
* Alerts(Job failure/Job slow)

## Applications

| application | responsibility |
| :---: | :---: |
| Map Reduce History Job Monitoring | parse mr history job logs from hdfs |
| Map Reduce Running Job Monitoring | get mr running job details from resource manager |
| Map Reduce Metrics Aggregation | aggregate metrics generated by applications above |

## Data Ingestion And Process

We build storm topology to fulfill requirements for each application.

![topology figures](include/images/jpm.jpg)

* Map Reduce History Job Monitoring (Figure 1)
    * **Read Spout**
        * read/parse history job logs from HDFS and flush to eagle service(storage is Hbase)
    * **Sink Bolt**
        * convert parsed jobs to streams and write to data sink
* Map Reduce Running Job Monitoring (Figure 2)
    * **Read Spout**
        * fetch running job list from resource manager and emit to Parse Bolt
    * **Parse Bolt**
        * for each running job, fetch job detail/job counter/job configure/tasks from resource manager
* Map Reduce Metrics Aggregation (Figure 3)
    * **Divide Spout**
        * divide time period(need to be aggregated) to small pieces and emit to Aggregate Bolt
    * **Aggregate Bolt**
        * aggregate metrics for given time period received from Divide Spout

## Integration With Alert Engine

In order to integrate applications with alert engine, follow below steps:

* **define stream**
    * define stream in resource/META-INF/providers/xxxProviders.xml
* **configure data sink**
    * currently, create kafka topic
* **emit stream data**
    * currently, write to kafka topic
* **define policy**
    * define policy in web ui and enable it, eagle server will schedule it
* **view alerts**
    * view in alerts page

Currently, Map Reduce History Job Monitoring has been integrated with alert engine. For example, if you want to receive map reduce job failure alerts, you can define policies (SiddhiQL) as the following:
```sql
from map_reduce_failed_job_stream[site=="sandbox" and currentState=="FAILED"]
select site, queue, user, jobType, jobId, submissionTime, trackingUrl, startTime, endTime
group by jobId insert into map_reduce_failed_job_stream_out
```

All columns above are predefined in stream map_reduce_failed_job_stream defined in

    eagle-jpm/eagle-jpm-mr-history/src/main/resources/META-INF/providers/org.apache.eagle.jpm.mr.history.MRHistoryJobApplicationProvider.xml

Then, enable the policy in web ui after it's created. Eagle will schedule it automatically.

---

# FAQ
`Placeholder for topic: FAQ`